---
title: "Modelos multinivel con enfoque bayesiano para la estimación de la población"
subtitle: "CEPAL - Unidad de Estadísticas Sociales"
format: 
  beamer: 
    colortheme: dove
    fonttheme: default
    incremental: false
    aspectratio: 1610
    #theme: Berkeley
    toc: true
    slide_level: 2
    #highlight: pygments
Email: andres.gutierrez@cepal.org
lang: es
editor_options:
  markdown:
    wrap: 90
---

```{r setup, include=FALSE}
library(printr)
library(ggplot2)
library(magrittr)
library(survey)
library(srvyr)
library(convey)
library(TeachingSampling)
library(printr)
library(furrr)
library(purrr)
library(tidyr)
library(knitr)
library(kableExtra)
library(bayesplot) 
library(posterior)
library(patchwork)
library(rstan)
library(bayesplot)
library(posterior)

select <- dplyr::select


knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE, 
  cache.path = "00_Cache/"

)
ggplot2::theme_set(theme_bw())
options(digits = 4)

tba <- function(dat, cap = NA) {
  kable(dat,
        format = "latex",
        digits =  4,
        booktabs=T,
      linesep="",
        caption = cap) %>%
    kable_styling(bootstrap_options = "striped", full_width = F) %>%
    kable_classic(full_width = F)
}


```


# Introducción al pensamiento bayesiano. 

## Modelos de población con el enfoque de **Tom**

Y te levantas un día...

-   Y te sientes un poco raro, y débil. Vas al médico y te hacen exámenes. Uno de ellos te marca positivo para una enfermedad muy rara que solo afecta al 0.1% de la población.

**No son buenas noticias.**

-   Vas al consultorio del médico y le preguntas qué tan específico es el examen. Te dice que es muy preciso; identifica correctamente al 99% de la gente que tiene la enfermedad.

## Y conoces a Thomas...

![](Recursos/00_Intro_bayes/01_Fig_Ton.PNG){width="500"}


## ¿cómo funciona?

![](Recursos/00_Intro_bayes/02_Fig_bayes.png){width="500"}

## ¿cómo funciona?

![](Recursos/00_Intro_bayes/03_Fig_bayes2.png){width="500"}


## Y pides una segunda opinión

-   Y esta vez el médico ordena que vuelves a realizarte ese mismo examen... y vuelves a marcar positivo para esa enfermedad.

- **Y vuelves a preguntarte:** _¿cuál es la probabilidad de que tenga esa enfermedad?_

Esta vez, has actualizado tu información sobre $Pr(E)$, pues ya marcaste positivo en un examen

$$
Pr(E)= 0.09 \ Y \ Pr(-E) = 0.91
$$

Por lo tanto:

$$
Pr(E \mid + +) = 0.997 \approx 91\%
$$


## Elementos de la regla de Bayes

En términos de inferencia para $\boldsymbol{\theta}$, es necesario encontrar la distribución de los parámetros condicionada a la observación de los datos. Para este fin, es necesario definir la distribución conjunta de la variable de interés con el vector de parámetros.

$$
p(\boldsymbol{\theta},\boldsymbol{Y})=p(\boldsymbol{\theta})p(\boldsymbol{Y} \mid \boldsymbol{\theta})
$$

-   La distribución $p(\boldsymbol{\theta})$ se le conoce con el nombre de distribución previa.

-   El término $p(\boldsymbol{Y} \mid \boldsymbol{\theta})$ es la distribución de muestreo, verosimilitud o distribución de los datos.

-   La distribución del vector de parámetros condicionada a los datos observados está dada por

$$
p(\boldsymbol{\theta} \mid \boldsymbol{Y})=\frac{p(\boldsymbol{\theta},\boldsymbol{Y})}{p(\boldsymbol{Y})}=\frac{p(\boldsymbol{\theta})p(\boldsymbol{Y} \mid \boldsymbol{\theta})}{p(\boldsymbol{Y})}
$$

## Regla de Bayes

-   El término $p(\boldsymbol{\theta} \mid \boldsymbol{Y})$ se le conoce con el nombre de distribución ***posterior***.  

-   El denominador no depende del vector de parámetros y considerando a los datos observados como fijos, corresponde a una constante y puede ser obviada. Luego, 

$$
    p(\boldsymbol{\theta} \mid \boldsymbol{Y})\propto p(\boldsymbol{Y} \mid \boldsymbol{\theta})p(\boldsymbol{\theta})
$$



## Distribución previa informativa para $\theta$

![](Recursos/00_Intro_bayes/04_Fig_bayes_previa.PNG){width="500"}


## Distribución previa NO informativa para $\theta$
![](Recursos/00_Intro_bayes/05_Fig_bayes_previa2.PNG){width="500"}



## Modelo de área Poisson 

Suponga que $\boldsymbol{Y}=\{Y_1,\ldots,Y_n\}$ es una muestra aleatoria de variables con distribución Poisson con parámetro $\theta$, la función de distribución conjunta o la función de verosimilitud está dada por
$$
p(\boldsymbol{Y} \mid \theta) = \prod_{i=1}^n\frac{e^{-\theta}\theta^{y_i}}{y_i!}I_{\{0,1,\ldots\}}(y_i)
$$

$$
= \frac{e^{-n\theta}\theta^{\sum_{i=1}^ny_i}}{\prod_{i=1}^ny_i!}I_{\{0,1,\ldots\}^n}(y_1,\ldots,y_n)
$$

donde $\{0,1\ldots\}^n$ denota el producto cartesiano $n$ veces sobre el conjunto $\{0,1\ldots\}$. 

El parámetro $\theta$ está restringido al espacio $\Theta=(0,\infty)$.  


## Distribución previa para $\theta$

-   La distribución previa del parámetro $\theta$ dada por

$$
p(\theta \mid \alpha,\beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1} e^{-\beta\theta}I_{(0,\infty)}(\theta).
$$

-   La distribución posterior del parámetro $\theta$ está dada por

$$
\theta \mid \boldsymbol{Y} \sim Gamma\left(\sum_{i=1}^ny_i+\alpha,n+\beta\right)
$$

# Base de datos  Censo de Costa Rica 

## Filtrado y Refinamiento de Datos del Censo

En el proceso de mejorar y depurar la base de datos del censo, es fundamental establecer reglas consistentes y replicables. En este contexto, el procedimiento de "Filtrado y Refinamiento de Datos del Censo" se vuelve esencial para mejorar la calidad de los datos y eliminar información irrelevante.

A continuación veremos el conjunto de filtros realizados para obtener un base de datos con información precisa y valiosa de los datos del censo.

## Lectura de Datos de Viviendas sin Coordenadas.

-   Importación de datos de viviendas desde un archivo CSV.

```{r, eval=FALSE}
Viviendas_sin_coordenadas <-
  read_csv2("Recursos/02_Census_Filters/Data/Viviendas sin coordenadas.csv")
```

-   Transformación de datos al formato requerido.

-   Creación de variables como ID de provincia, cantón y distrito.

```{r, eval = FALSE}
Viviendas_sin_coordenadas %<>% 
  transmute(
    LLAVEV,
    PROV_ID = str_sub(CODIGO_PCD, 1,1),
    CANT_ID = str_sub(CODIGO_PCD, 1,3),
    DIST_ID    = as.character(CODIGO_PCD),
    UGM_ID = paste0(CODIGO_PCD , ID_UGM), 
    H01A_TOTAL_PERSONAS = H01A_TOTAL_RESIDENTES_HAB)
```


## Lectura de Datos del Censo Estandarizado.

Leer los datos del censo estandarizado desde un archivo RDS (R Data Serialization) almacenado. De manera similar a los pasos anteriores, ajustamos los códigos del UGM para mantener la consistencia de los datos. 

```{r, eval=FALSE}
censo1 <- 
  readRDS(
    "Recursos/02_Census_Filters/Data/censo_estandarizado.rds") 
```

## Incorporación de la Base de Edad y Sexo.

En esta sección, incorporamos la base de edad y sexo en el análisis. La base de edad y sexo se lee desde un archivo RDS almacenado. 

```{r, eval=FALSE}
censo_sexo_edad <-
  readRDS(
    "Recursos/02_Census_Filters/Data/Censo con grupos por sexo.rds") 
```

## Unión Interna para Agregar la Base de Edad y Sexo

- Cálculo de la diferencia en el recuento de filas entre las bases.

- Identificación de hogares censados en papel no incluidos.

- Comparación del número de filas con los datos del censo y viviendas sin coordenadas.
   
```{r, eval=FALSE}
censo1 <- inner_join( censo1, censo_sexo_edad,
  join_by( un_ID, PROV_ID,  CANT_ID,
    DIST_ID,  UGM_ID, LLAVEV,  V01_TIPO_VIVIENDA,
    V02_OCUPACION_VIVIENDA
  )
)

```

## Aplicación del primer filtro 

**Categorización de hogares con residentes y determinación del estado de greenpoint**

  1. Creamos una nueva columna llamada 'personas' para categorizar los hogares como con residentes ('si') o vacíos ('no') según el número total de residentes en cada hogar.

  2. Se determina el estado de 'greenpoint' de cada hogar en función de condiciones específicas. Si el valor de 'greenpoint' es '0' y el valor de 'personas' es 'si', asignamos un valor de '1' a la columna 'greenpoint'.

**greenpoint:** La casa está censada en el mapa de puntos.

## Análisis de la distribución de greenpoints

  1. Se calcula la distribución del estado de greenpoint entre los hogares.

  2. Agrupa los datos según el estado de 'greenpoint' y cuenta el número de hogares en cada categoría.

  3. Calcula la distribución porcentual para cada categoría.



```{r, eval=TRUE, echo=FALSE}
readRDS("Recursos/01_Census_Filters/greenpoint_distribution.rds") %>% 
  tba()
```

## Características de los hogares según el estado de greenpoint:

En la siguiente tabla se resumen las características de los hogares en función de su estado de greenpoint. Calcula el número mínimo y máximo de residentes en los hogares, cuenta los valores faltantes para el número total de residentes y proporciona el recuento total de hogares para cada categoría de greenpoint.

```{r, eval = TRUE, echo = FALSE}
readRDS("Recursos/01_Census_Filters/household_summary.rds") %>% 
  tba()
```

## Validación de entre la ocupación y el estado de greenpoint:

Se genera una tabla de contingencia para explorar la relación entre la ocupación y el estado de greenpoint.  Esto proporciona una representación visual de cómo se distribuyen estas dos variables entre los hogares.


```{r, eval = TRUE, echo = FALSE}
readRDS("Recursos/01_Census_Filters/occupancy_greenpoint_table.rds") %>% tba()
```

## Aplicación del Segundo Filtro

Aplicamos el segundo filtro para categorizar aún más los hogares en función de criterios adicionales. Introducimos el estado de "greenpoint2" para describir detalladamente el estado de los hogares basándonos en diferentes criterios, como el número de residentes, los resultados de las entrevistas y la ocupación de la vivienda.

## Codígo para la aplicación del Segundo Filtro

```{r, eval = FALSE, echo = TRUE}
censo2 <- censo1 %>% mutate(
  greenpoint2 = case_when(
    
    H01A_TOTAL_PERSONAS > 0 ~ "Censado con informacion n>0",
    
    RESUL_ENTREVISTA_VIV %in% c(1) &
      H01A_TOTAL_PERSONAS == 0 ~ "Censado con informacion n=0",
    
    RESUL_ENTREVISTA_VIV %in% c(3, 4) ~ "Sin informacion pero  n>0",
    
    is.na(greenpoint) & is.na(personas) ~ "Sin informacion pero n>=0",
    
    V02_OCUPACION_VIVIENDA == "8" ~ "Sin informacion pero n>=0",
    
    TRUE ~ "Resto"
  )
)
```


## Aplicación del segundo filtro: Criterio WorldPop

Incluimos todos los hogares con la variable WorldPop (WP) que se encuentran dentro de 1 desviación estándar de su valor promedio. Sin embargo, si estos hogares tienen cero residentes en la variable de interés, marcamos esa variable como "No Disponible" (NA).

Las  estadísticas resumen para la variable 'wpop_sum' se muestran en la siguiente tabla.

```{r, echo = FALSE, eval = TRUE}
readRDS("Recursos/01_Census_Filters/wpop_summary.rds") %>% tba()
```

Estas estadísticas nos ayudan a establecer los umbrales para el filtro y se guardan en un archivo de resumen.


## Cálculo de umbrales inferiores y superiores

Utilizamos las estadísticas resumen para calcular los umbrales inferiores y superiores.

```{r, echo = TRUE, eval=FALSE}
li <- 96.96515 - 143.1986 * 1  # Umbral inferior
ls <- 96.96515 + 143.1986 * 1  # Umbral superior
```

Identificamos y contamos los hogares que cumplen con los criterios del segundo filtro. Nos enfocamos en los hogares con cero residentes ('H01A_TOTAL_PERSONAS') pero que tienen valores de 'wpop_sum' fuera de los umbrales calculados.

## Aplicando los umbrales inferiores y superiores
Para validar el resultado de aplicar el criterio de WorldPop se realiza la siguiente tabla. 

```{r, echo = TRUE, eval=FALSE}
filter_2_counts <- censo2 %>% 
  filter(H01A_TOTAL_PERSONAS == 0, wpop_sum > ls | wpop_sum < li) %>%
  group_by(V02_OCUPACION_VIVIENDA) %>% summarise(n = n())
filter_2_counts
```


```{r, echo = FALSE, eval=TRUE}
readRDS("Recursos/01_Census_Filters/05_filter_2_counts.rds") %>% tba()
```

## Aplicación del segundo filtro y actualización de columnas

Finalmente, aplicamos el segundo filtro a los hogares y actualizamos las columnas 'greenpoint2' y 'Filtros'.

```{r, echo = TRUE, eval=FALSE}
censo3 <- censo2 %>% mutate(
  greenpoint2 = case_when(
    H01A_TOTAL_PERSONAS == 0 & 
      (wpop_sum > ls | wpop_sum < li)  ~ "Sin información pero n>=0",
    TRUE ~ greenpoint2
  ),
  Filtros = case_when(
    H01A_TOTAL_PERSONAS == 0 &
      (wpop_sum > ls | wpop_sum < li)  ~ "Criterio WorldPop",
    TRUE ~ NA_character_
  )
)
```

## Resumen de datos basados en 'greenpoint2'

Resumimos los datos basados en la variable 'greenpoint2' actualizada. Estos datos nos ayudan a comprender el impacto del filtro en la clasificación de los hogares.

```{r, echo = FALSE, eval = TRUE}
readRDS(file = "Recursos/01_Census_Filters/summary_greenpoint2.rds") %>% tba()
```


## Resumen de estadísticas basadas en 'greenpoint2

- Calculamos estadísticas adicionales para las categorías de 'greenpoint2'. 

- Estos datos son esenciales para comprender la distribución de residentes en los hogares filtrados.


```{r, echo = FALSE, eval = TRUE}
readRDS("Recursos/01_Census_Filters/summary_greenpoint2_stats.rds") %>% tba()
```

## Definición del Tercer Filtro

- El tercer filtro aborda los hogares dentro de las UGM que fueron encuestados después de un intervalo mayor a 20 días y, a pesar de estar clasificados como desocupados, existe incertidumbre sobre su estado de ocupación. 

- Para la implementar el filtro se contaba con el archivo 'Desocupadas fuera periodo.xlsx' que recopila información sobre los hogares que estaban desocupados pero fueron visitados fuera del intervalo estándar. 

- Utilizando la información recopilada actualizamos las columnas 'greenpoint2' y 'Filtros' según los criterios especificados.

## Implementando el tercer filtro

```{r, echo = TRUE, eval=FALSE}
censo4 <- censo3 %>% mutate(
  greenpoint2 = case_when(
    UGM_ID %in% upms_reporte$UGM_ID & 
      H01A_TOTAL_PERSONAS == 0  ~ "Sin informacion pero n>=0",
    TRUE ~ greenpoint2
  ),
  Filtros = case_when(
    UGM_ID %in% upms_reporte$UGM_ID & 
      H01A_TOTAL_PERSONAS == 0  ~ "Fuera de periodo (20 días)",
    TRUE ~ Filtros
  )
)
```

##  Aplicación de filtros adicionales y creación de valores en 'Filtros'

Refinando aún más los datos mediante la aplicación de filtros adicionales. Los valores en 'Filtros' se actualizan en función de diversas condiciones, como el número de residentes, el resultado de la entrevista ('RESUL_ENTREVISTA_VIV') y la ocupación de la vivienda ('V02_OCUPACION_VIVIENDA').

\small
```{r, echo = TRUE, eval=FALSE}

censo4 %<>% mutate(Filtros = case_when(
is.na(Filtros) & H01A_TOTAL_PERSONAS > 0 ~ "Número de personas mayor a 0",
  
is.na(Filtros) & RESUL_ENTREVISTA_VIV %in% c(1) &
 H01A_TOTAL_PERSONAS == 0 ~ "Entrevista igual a 1 y número de personas igual a 0",
  
is.na(Filtros) & RESUL_ENTREVISTA_VIV %in% c(3,4) ~ "Entrevista es 3 o 4",
  
is.na(Filtros) & is.na(greenpoint) & is.na(personas) ~ "Sin conteo de personas",

is.na(Filtros) & V02_OCUPACION_VIVIENDA == "8" ~ "Ocupación de la vivienda es 8", 

  TRUE ~ Filtros
))
```


## Resumen de datos basados en la variable 'greenpoint2'

Este resumen nos ayuda a comprender el impacto del tercer filtro en la clasificación de los hogares.

```{r, echo = FALSE, eval=TRUE}
readRDS("Recursos/01_Census_Filters/summary_greenpoint2.rds") %>% 
  tba()
```


## Resumen de datos basados en 'greenpoint2' y 'Filtros'

Generamos un resumen adicional que considera la combinación de las variables 'greenpoint2' y 'Filtros'. Esto proporciona información más detallada sobre cómo el Criterio WorldPop afecta a las categorías existentes.

\scriptsize

```{r, echo = FALSE, eval = TRUE}
readRDS("Recursos/01_Census_Filters/summary_greenpoint2_filtros.rds") %>% tba()
```

**En un proceso adicional incorporamos las entrevistas realizadas en papel y se hace la validación de identificadores duplicados** 

## Tabla de resumen finales 

\tiny

```{r, eval=TRUE, echo=FALSE}
readRDS(file = "Recursos/01_Census_Filters/summary2.rds") %>% 
  tba()
```



## Proceso de estimación en **STAN**

Sea $Y$ el conteo de personas censadas por UGM del país. Aquí incluimos las viviendas con cero personas.  

```{r,echo=TRUE}
dataPois <- 
  readRDS("Recursos/00_Intro_bayes/Poisson/conteo_viviendas.rds")
```

```{r,echo=FALSE}
tba(dataPois %>% head(8), cap = "Conteno de personas y viviendas" )
```

## Histrograma con el conteno de personas

![](Recursos/00_Intro_bayes/Poisson/02_Hist_pois.png){width="500"}


## Modelo escrito en código `STAN`
\small
```
data {
  int<lower=0> n;      // Número de áreas geograficas 
  int<lower=0> y[n];   // Conteos por area
  real<lower=0> alpha;
  real<lower=0> beta;
}
parameters {
  real<lower=0> theta;
}
model {
  y ~ poisson(theta);
  theta ~ gamma(alpha, beta);
}
generated quantities {
    real ypred[n];                    // vector de longitud n
    for(ii in 1:n){
    ypred[ii] = poisson_rng(theta);
    }
}

```

## Preparando datos para código `STAN`

-   Organizando datos para STAN

```{r,eval=FALSE}
sample_data <- list(n = nrow(dataPois), y = dataPois$tot_personas, 
                    alpha = 0.001, beta = 0.001)
```

-   Ejecutando el código de `STAN`

```{r, eval=FALSE}
stan_pois <- "Recursos/00_Intro_bayes/Poisson/03_Poisson.stan"
model_poisson <-
  stan(
    file = stan_pois, data = sample_data,
    warmup = 500,
    iter = 1000,
    verbose = FALSE,   cores = 4
  )
saveRDS(model_poisson,
           "Recursos/00_Intro_bayes/Poisson/model_poisson.rds")

```

## Resultados de la estimación del  parámetro $\theta$

```{r,eval=FALSE}
model_poisson <- readRDS(
           "Recursos/00_Intro_bayes/Poisson/model_poisson.rds")
tabla_posi <- summary(model_poisson,
                      pars = c("theta"))$summary
tabla_posi %>% tba()
saveRDS(tabla_posi, "Recursos/00_Intro_bayes/Poisson/04_tabla_theta.rds")
```
\small
```{r, echo=FALSE}
tabla_posi <- readRDS("Recursos/00_Intro_bayes/Poisson/04_tabla_theta.rds")
tabla_posi %>% tba() 
```

## Convergencias de las cadenas el parámetro $\theta$

```{r,eval=FALSE}
posterior_theta <- as.array(model_poisson, pars = "theta")
p1 <- (mcmc_dens_chains(posterior_theta) +
    mcmc_areas(posterior_theta) ) / mcmc_trace(posterior_theta)
```

![Cadenas evaluación de las convergencias  de $\theta$](Recursos/00_Intro_bayes/Poisson/04_Pois_theta.png){width="350"}



## Chequeo predictivo posterior

```{r, eval=FALSE}
y_pred_B<-as.array(model_poisson,pars ="ypred") %>% 
  as_draws_matrix()

rowsrandom<-sample(nrow(y_pred_B),300)

y_pred2<-y_pred_B[rowsrandom,]

p1<- ppc_dens_overlay(y =as.numeric(dataPois$tot_personas), y_pred2)
p1 + xlim(0,300)

```

## Chequeo predictivo posterior

![Chequeo predictivo posterior para el total de personas por UGM](Recursos/00_Intro_bayes/Poisson/05_ppc.png){width="400"}

## Proceso de estimación en **STAN** (total de viviendas)

- Sea $Y$ el conteo de viviendas ocupadas con personas presentes censadas por UGM.  

**Empleando un proceso igual que el caso anterior se realiza la estimación del modelo para la variable $Y$**

-   Organizando datos para STAN

```{r,eval=FALSE}
sample_data <- list(n = nrow(dataPois), y = dataPois$tot_viviendas, 
                    alpha = 0.001, beta = 0.001)
```

## Ejecutando el código de `STAN`

```{r, eval=FALSE}
stan_pois <- "Recursos/00_Intro_bayes/Poisson/03_Poisson.stan"
model_poisson_vivi <-
  stan(
    file = stan_pois, data = sample_data,
    warmup = 500,
    iter = 1000,
    verbose = FALSE,   cores = 4
  )
saveRDS(model_poisson_vivi,
           "Recursos/00_Intro_bayes/Poisson/model_poisson_vivi.rds")

```


## Resultados de la estimación del  parámetro $\theta$

\small
```{r, echo=FALSE}
tabla_posi <- readRDS("Recursos/00_Intro_bayes/Poisson/06_tabla_theta_vivi.rds")
tabla_posi %>% tba() 
```


![Evaluación de la convergencias  de $\theta$](Recursos/00_Intro_bayes/Poisson/07_Pois_theta_vivi.png){width="350"}


## Chequeo predictivo posterior

![Chequeo predictivo posterior para el total de viviendas por UGM](Recursos/00_Intro_bayes/Poisson/08_ppc_vivi.png){width="400"}

# Estandarización y validación de covariables



## Estandarización y validación de covariables

  - De manera similar a como se validaron las variables del censo, el conjunto de covariables pasa por un proceso de validación. Esto implica asegurar la uniformidad en la longitud de identificadores como UGM, Cantos, regiones, etc.

  - Se lleva a cabo una validación para identificar valores faltantes (NAs) en el conjunto de datos.

  - Posteriormente, se realiza un análisis descriptivo de los datos.


## Estandarización y validación de covariables

  - Comenzamos creando un resumen que incluye los nombres de las columnas y sus tipos de datos correspondientes.
  
  - Luego, mejoramos este resumen añadiendo una columna que indica el tipo de datos de cada columna. Seguido de esto se crean las estadísticas de resumen como promedios, desviación estándar, máximos, mínimos y otros. 
  
- El proceso continua con la estandarización del tipo de información (carácter o numérico)    

   **Numéricas**: Se estandarizan a escala de media cero y desviación estándar 1 
   **Carácter**: La longitud de los caracteres sea igual dentro de la variable. 

## Valores descriptivos de la base de UGM

```{r, echo=FALSE}
resumen2 <- readRDS("Recursos/02_Input_Validation_Cov/01_resumen_ugm.rds")
resumen2 %>% filter(tipo == "character")  %>% 
  select(Nombre = Nombre_Columna,
         Num_nas_char:leng_max) %>% 
  tba(cap = "Valores descriptivos de la base de UGM (Carácter)") 
```
  
## Valores descriptivos de la base de UGM
\tiny
```{r, echo=FALSE}
resumen2 %>% filter(tipo != "character")  %>% 
  select(Nombre = Nombre_Columna,
         Num_nas:Valor_Maximo) %>% 
  tba(cap = "Valores descriptivos de la base de UGM (Numéricas)") 
```

# Modelo multinivel para censos   

## Caso de estudio Costa Rica 

- Los modelos que se implementaron,  aprovecharon una variedad de fuentes de datos, como el pre-censo, el censo, registros administrativos e información geoespacial. Que fue preparada previamente. 

- La información geoespacial se ajustó según las unidades geoestadísticas mínimas (UGM), que desempeñaron un papel clave como sectores censales y áreas de empadronamiento.

- Se implementó un modelo bayesiano similar a los del Proyecto WorldPop de la Universidad de Southampton en Inglaterra para predecir el número de personas en viviendas no visitadas, ocupadas pero con habitantes ausentes o que rechazaron responder al cuestionario censal.
- Este modelo se basa en la suposición de que el número de personas en una vivienda de una UGM sigue una distribución de Poisson.

## Caso de estudio Costa Rica 

La ecuación básica del modelo es:  

\begin{eqnarray*}
\ensuremath{Y_{ij}} & \sim & Poisson\left(N_{ij}\times D_{j}\right)\\
\log\left(D_{j}\right) & = & x_{ij}^{t}\beta+z_{ij}^{t}\gamma\\
\beta & \text{\ensuremath{\sim}} & N\left(0,10000\right)\\
\gamma & \text{\ensuremath{\sim}} & N\left(0,10000\right)
\end{eqnarray*}


donde $Y_{ij}$ es el número de personas que habitan en la i-ésima vivienda de la j-ésima UGM, $N_{ij}$ es el número de viviendas en esa UGM (conocido desde el censo y el precenso), $D_j$ es la densidad poblacional promedio en la UGM.

Note que $D_j$ se relaciona por medio de la función logaritmo con los correspondientes efectos fijos $x^t_{ij}\beta$ y los efectos aleatorios $z^t_{ij}\gamma$


## Obejtivo.

Estimar el total de personas en Costa Rica, es decir, 

$$
Y = \sum_{U}Y_{d}
$$
donde $Y_{d}$ es total de personas en la $d$-ésima UGM

Note que, 


\begin{equation*}
Y =  \sum_{U_d}Y_{d} + \sum_{U^c_d}Y_{d} 
\end{equation*}

## Estimador de $Y$

El estimador de $Y$ esta dado por: 

$$
\hat{Y}_d = \sum_{U_d}Y_{d} + \sum_{U^c_d}\hat{y}_{d}
$$

donde

$$\hat{y}_{d}=E_{\mathscr{M}}\left(Y_{d}\mid\boldsymbol{X}_{d},\boldsymbol{\beta}\right)$$,

donde $\mathscr{M}$ hace referencia a la medida de probabilidad inducida por el modelamiento. De esta forma se tiene que, 

$$
\hat{Y}_d = \sum_{U_{d}}\hat{y}_{di}
$$




## Modelo en `Stan` 

```
data {
  int<lower=1> D; // Número de UGMs 
  int<lower=1> K; // Cantidad de regresores
  int<lower=1> Kz; // Cantidad de efectos aleatorios
  int<lower=0> Y_obs [D]; // conteos de poblacion por UGM 
  int<lower=0> V_obs [D]; // Número de viviendas censadas 
  matrix[D, K] X_obs; // matriz de covariables
  matrix[D, Kz] Z_obs; // matriz de dummis
}

parameters {
  vector[K] beta; // matriz de parámetros 
  vector[Kz] gamma; // Efectos aleatorios 
  real<lower=0> densidad [D]; 
  real<lower=0> sigma;
}
```
## Modelo en `Stan` 

```
transformed parameters {
  real<lower=0> lambda [D];
  vector[D] lp; // vector de parámetros
  
  lp = X_obs * beta + Z_obs * gamma;
  for(d in 1:D){
     lambda[d] = densidad[d] * V_obs[d];
  }
 
}
```
## Modelo en `Stan` 

```
model {
  // Prior
  gamma ~ normal(0, 10);
  beta ~ normal(0, 1000);
  sigma ~ inv_gamma(0.001, 0.001);
  
  // Likelihood
  for (d in 1:D) {
    Y_obs[d] ~ poisson(lambda[d]);
  }
  
  // Log-normal distribution for densidad
  for (d in 1:D) {
    densidad[d] ~ lognormal(lp[d], sigma);
  }
}

```


## Consideraciones para los modelos. 

Durante el proceso de validación de la información censal se hizo la clasificación de los registros en 4 categorías, que debemos utilizar para generar resultados considerando estas clasificaciones: 

*Modelo 1:* Considera las viviendas con información (Ocupadas y desocupadas).

- Censado con información n=0

- Censado con información n>0

*Modelo 2:* Considera las viviendas con personas presentas

- Censado con información n>0

## Listado de variables usadas en el modelo 

- ugm_peligrosidad                         
- ugm_problema_de_acceso                   
- ugm_riesgos_amenazas                     
- ugm_cobertura_telecomunicaciones         
- dist_permisos_de_construccion_2011_2022  
- dist_poblacion_proyeccion_ajustada_2022  
- dist_poblacion_ccss_abril_2023           
- dist_matricula_educacion_primaria_2021   
- GHS_BUILT_S_E2020_GLOBE_R2023A_5367_CRI (superficie urbanizada en m2.) 
- urban_coverfraction                      
- crops_coverfraction  

## Listado de variables usadas en el modelo 

- asent   (Asentamientos Informales)                                 
- ppp_CRI_v2  (Densidad de población en modelo Wpop)                             
- elev (Altitud del área en metros sobre el nivel del mar)                                  
- indig   (Define si un área esta dentro o fuera de un area indigena)                                 
- aprot   (Define si un área esta dentro o fuera de un area protegida)
- ebais_tt  (Tiempo de viaje al EBAI mas próximo en minutos)                                
- escu_tt    (Tiempo de viaje a la escuela mas próximo en minutos)                              
- igl_tt     (Tiempo de viaje a la iglesia  as próximo en minutos)                               
- dist_nl_mean 


## Proceso de estimación de los modelos en  `R` y `STAN`

Lectura de la información censal y las covariables que fueron previamente estandarizada y debidamente organizada. 

```{r,eval = FALSE}
censo_vivienda <- 
  readRDS("Recursos/03_Model_for_people/censo_viviendas.rds") 
Base_ugms <- 
  readRDS("Recursos/03_Model_for_people/Base_ugms_estandarizada.rds")
```

 **Seleccionado los datos para el Modelo 1** 

```{r, eval=FALSE}
base_ugm_estima_todas <- censo_vivienda %>% 
  filter( !greenpoint2 %in% c("Sin informacion pero  n>0", 
                              "Sin informacion pero n>=0")) %>% 
  group_by(UGM_ID) %>% 
  summarise(tot_personas = sum(H01A_TOTAL_PERSONAS),
            tot_viviendas = n())
base_ugm_estima_todas <-
  inner_join(base_ugm_estima_todas, Base_ugms, by = "UGM_ID") 
```

## Preparando datos para `STAN`

```{r, eval=FALSE}
Y_obs <- base_ugm_estima_todas$tot_personas
N_obs <- base_ugm_estima_todas$tot_viviendas
```

**Efectos aleatorio** 

```{r, eval=FALSE}
Z_obs <- model.matrix(UGM_ID ~
                        -1 +
                        (PROV_ID) +
                        (CANT_ID) +
                        (DIST_ID)  ,
                      data = base_ugm_estima_todas)
```



## Efectos Fijos

```{r, eval=FALSE}
X_obs <- model.matrix( UGM_ID~ dist_codigo_urbanidad +
 ugm_peligrosidad + ugm_problema_de_acceso +                  
 ugm_riesgos_amenazas + ugm_cobertura_telecomunicaciones +        
 dist_permisos_de_construccion_2011_2022 + 
 dist_poblacion_proyeccion_ajustada_2022 + 
 dist_poblacion_ccss_abril_2023 +          
 dist_matricula_educacion_primaria_2021 + dist_codigo_urbanidad +
 GHS_BUILT_S_E2020_GLOBE_R2023A_5367_CRI + 
 urban_coverfraction + crops_coverfraction + asent +
 ppp_CRI_v2 + elev + indig + aprot + ebais_tt +                                
 escu_tt + igl_tt + dist_nl_mean,
 data = base_ugm_estima_todas
) %>% as.matrix()

```

## Definiendo el sample_data para `STAN`

```{r, eval=FALSE}
sample_data <- list(
  D = nrow(X_obs) , # Número de UGM 
  K = ncol(X_obs),  # Número de efectos fijos
  Kz = ncol(Z_obs), # Número de efectos aleatorios
  Y_obs = Y_obs,    # Conteo de personas por UGM
  V_obs = N_obs,    # Conteo de personas Por UGM
  X_obs = X_obs %>% as.matrix(), 
  Z_obs = Z_obs %>% as.matrix()
)
```

## Ejecutando el modelo en `STAN`

```{r,eval=FALSE}
options(mc.cores = parallel::detectCores())
rstan::rstan_options(auto_write = TRUE) # speed up running time

fit_poisson_todas <- stan(
  file = "Recursos/03_Model_for_people/02_Modelo_worldpop.stan",
  # Stan program
  data = sample_data, # named list of data
  verbose = TRUE, 
  warmup = 10000, # number of warmup iterations per chain
  iter = 15000,   # total number of iterations per chain
  cores = 4,      # number of cores (could use one per chain)
)

saveRDS(fit_poisson_todas,
        "Recursos/03_Model_for_people/fit_poisson_todas_worldpop.rds")

```

## Seleccionado los datos para el Modelo 2 

```{r, eval=FALSE}
base_ugm_estima_ocupadas <- censo_vivienda %>%
  filter(greenpoint2 %in% c("Censado con informacion n>0",
                            "Papel n>0")) %>%
  group_by(UGM_ID) %>%
  summarise(tot_personas = sum(H01A_TOTAL_PERSONAS),
            tot_viviendas = n())

base_ugm_estima_ocupadas <-
  inner_join(base_ugm_estima_ocupadas,
             Base_ugms, by = "UGM_ID") 
```

## Preparando datos para `STAN`

```{r, eval=FALSE}
Y_obs <- base_ugm_estima_ocupadas$tot_personas
N_obs <- base_ugm_estima_ocupadas$tot_viviendas
```

**Efectos aleatorio** 

```{r, eval=FALSE}
Z_obs <- model.matrix(UGM_ID ~
                        -1 +
                        (PROV_ID) +
                        (CANT_ID) +
                        (DIST_ID)  ,
                      data = base_ugm_estima_ocupadas)
```



## Efectos Fijos

```{r, eval=FALSE}
X_obs <- model.matrix( UGM_ID~ dist_codigo_urbanidad +
 ugm_peligrosidad + ugm_problema_de_acceso +                  
 ugm_riesgos_amenazas + ugm_cobertura_telecomunicaciones +        
 dist_permisos_de_construccion_2011_2022 + 
 dist_poblacion_proyeccion_ajustada_2022 + 
 dist_poblacion_ccss_abril_2023 +          
 dist_matricula_educacion_primaria_2021 + dist_codigo_urbanidad +
 GHS_BUILT_S_E2020_GLOBE_R2023A_5367_CRI + 
 urban_coverfraction + crops_coverfraction + asent +
 ppp_CRI_v2 + elev + indig + aprot + ebais_tt +                                
 escu_tt + igl_tt + dist_nl_mean,
 data = base_ugm_estima_ocupadas
) %>% as.matrix()

```

## Definiendo el sample_data para `STAN`

```{r, eval=FALSE}
sample_data <- list(
  D = nrow(X_obs) , # Número de UGM 
  K = ncol(X_obs),  # Número de efectos fijos
  Kz = ncol(Z_obs), # Número de efectos aleatorios
  Y_obs = Y_obs,    # Conteo de personas por UGM
  V_obs = N_obs,    # Conteo de personas Por UGM
  X_obs = X_obs %>% as.matrix(), 
  Z_obs = Z_obs %>% as.matrix()
)
```

## Ejecutando el modelo en `STAN`

```{r,eval=FALSE}
options(mc.cores = parallel::detectCores())
rstan::rstan_options(auto_write = TRUE) # speed up running time

fit_poisson_todas <- stan(
  file = "Recursos/03_Model_for_people/02_Modelo_worldpop.stan",
  # Stan program
  data = sample_data, # named list of data
  verbose = TRUE, 
  warmup = 10000, # number of warmup iterations per chain
  iter = 15000,   # total number of iterations per chain
  cores = 4,      # number of cores (could use one per chain)
)

saveRDS(fit_poisson_todas,
        "Recursos/03_Model_for_people/fit_poisson_ocupadas_worldpop.rds")

```

## Pasos para la  predicción de la población 


- Después de esperar un tiempo prudente (15 días o más por modelo) se procede a obtener $\hat{y}_{d}=E_{\mathscr{M}}\left(Y_{d}\mid\boldsymbol{X}_{d},\boldsymbol{\beta}\right)$ para cada UGM  para cada modelo. 

- La predicción por UGM se hace siguiendo las siguientes reglas 

\begin{eqnarray*}
\hat{Y}_{d} & = & \begin{cases}
\hat{y}_{mod1} & \text{greenpoint2 == "Sin informacion pero n\ensuremath{\ge}0"}\\
\hat{y}_{mod2} & \text{greenpoint2 == "Sin informacion pero n>0"}\\
Y_{d} & \text{en otro caso}
\end{cases}
\end{eqnarray*}

- Siguiendo una regla similar se le asignas los Margenes de Error (ME) 

\begin{eqnarray*}
\hat{Y}^{ME}_{d} & = & \begin{cases}
\hat{y}^{ME}_{mod1} & \text{greenpoint2 == "Sin informacion pero n\ensuremath{\ge}0"}\\
\hat{y}^{ME}_{mod2} & \text{greenpoint2 == "Sin informacion pero n>0"}\\
0 & \text{en otro caso}
\end{cases}
\end{eqnarray*}

## Estimaciones agregadas 

- Ahora se presenta la metodología utilizada para obtener estimaciones en varios niveles de agregación, empleando un conjunto de funciones personalizadas diseñadas para este conjunto de datos en particular.

- Estas funciones facilitan el proceso de generar predicciones y visualizaciones, lo que nos permite analizar de manera efectiva las estimaciones de población.

```{r,eval=FALSE}
source("Recursos/03_Model_for_people/01_Funcion_agregados.R")
censo_vivienda <- 
  readRDS("Recursos/03_Model_for_people/05_censo_vivienda_personas_grupo_edad.rds")
```


  - **plot_densidad**: Esta función genera un gráfico de la densidad de una distribución normal con la media y la desviación estándar especificadas. Además, resalta un intervalo específico de la distribución con un área sombreada y segmentos en el gráfico.

  - **Pred_totPob**: Esta función realiza cálculos y visualizaciones relacionados con los datos de población total en un censo.


## Predicción del total poblacional

```{r, eval=FALSE}
p1 <- Pred_totPob(censo_vivienda, agrega = NULL, Plot = TRUE)
```
![](Recursos/03_Model_for_people/06_Fig_Nacional.png){width="300"}
```{r,echo=FALSE}
readRDS("Recursos/03_Model_for_people/05_tab_Nacional.rds") %>% 
  tba()

```




## Predicción del total por provincia

```{r,eval=FALSE}
tab <- Pred_totPob(censo_vivienda, agrega = "PROV_ID",  Plot = FALSE)
```
```{r,echo=FALSE}
readRDS("Recursos/03_Model_for_people/07_tab_prov.rds") %>% 
  tba()

```

## Predicción del total por provincia

```{r,eval=FALSE}
p1 <- Pred_totPob(censo_vivienda, agrega = "PROV_ID", filtro = "1",
                     Plot = TRUE)
```
![](Recursos/03_Model_for_people/08_Fig_Prov.png){width="350"}

## Predicción del total por distrito

```{r,eval=FALSE}
p1 <- Pred_totPob(censo_vivienda,  agrega = "DIST_ID",
                     Plot = TRUE,  filtro = "10110")

```
![](Recursos/03_Model_for_people/09_Fig_Dist.png){width="350"}

## Modelo multinomial para estimar personas por rango de edad y sexo


En el paso final, modelamos la estructura demográfica de las Unidades Geográficas Pequeñas a nivel de provincia. Esta estructura consta de 40 subgrupos basados en la combinación de género y grupos de edad. Para estimar la demografía de hogares no observados, utilizamos la predicción condicional basada en los resultados del paso anterior.

## Modelo multinomial para estimar personas por rango de edad y sexo

Dada la naturaleza del fenómeno en estudio, se considera adecuada una distribución multinomial para el recuento de individuos en los 40 grupos. el modelo utilizado es el siguiente: 

- $G_i$ sigue una distribución Multinomial $Multinomial(p_{G_i})$.

- Las log-odds de $p_{G_i}$ están modeladas por $x_i^T\beta$.

En este contexto, $G_i$ representa el recuento de cada uno de los cuarenta grupos demográficos. $p_{G_i}$ es un vector de tamaño 40 que contiene las probabilidades de que una persona clasificada en cada grupo resida en un hogar.

## Modelo multinomial: Procesamiento en `R`

El código elimina todos los objetos del entorno de R actual, asegurando un punto de partida limpio para las operaciones posteriores.

```{r}
library(tidyverse)   # Data manipulation and visualization
library(data.table)  # Fast data manipulation
library(openxlsx)    # Reading Excel files
library(magrittr)    # Pipe operator
library(lme4)        # For fitting linear mixed-effects models
library(nnet)        # For fitting neural networks
cat("\f")  # Clears console output
```

## Modelo multinomial: Lectura de datos en `R`

Para la lectura de los datos hacemos uso del siguiente código

```{r, eval=FALSE}
censo_vivienda <- 
  readRDS("Recursos/04_Multinomial/02_censo_vivienda_personas.rds")
```

Ahora, filtrar los datos del censo para conservar columnas relevantes para el análisis de edad y género.

```{r,eval=FALSE}
censo_vivienda_age_gender <- censo_vivienda %>% 
  filter( !greenpoint2 %in% c("Sin informacion pero  n>0", 
                              "Sin informacion pero n>=0")) %>% 
  dplyr::select(PROV_ID, HOMBRES_GRUPO1_sum:MUJERES_GRUPO20_sum)
```

## Modelo multinomial: Preparación de datos:

El código crea un nuevo conjunto de datos, censo_vivienda_edad_sexo, filtrando entradas específicas de censo_vivienda. Se seleccionan columnas relacionadas con grupos de edad y género y provincias. Luego, este conjunto de datos filtrado se agrega a nivel de PROV_ID utilizando la función `summarise_if.`
    

```{r,eval=FALSE}
# Summarizing the age and gender data by grouping 
# it based on province (PROV_ID).
censo_personas <- censo_vivienda_age_gender %>% 
  group_by(PROV_ID) %>% 
  summarise_if(is.numeric, sum)
```

## Modelo multinomial

Se crea un modelo multinomial utilizando la función `multinom`. Este modelo predice la distribución de grupos de edad y género dentro de los hogares según la provincia (`PROV_ID`). El modelo se almacena en la variable `model`.

```{r, eval=FALSE}
# Fitting a multinomial model to estimate the 
# distribution of age and gender within households,
# using province as the predictor.
model <- multinom(
  censo_personas[,-1] %>% as.matrix() ~ censo_personas$PROV_ID)

saveRDS(model, "Recursos/04_Multinomial/Multinomial_model.rds")
```

## Modelo multinomial: Predicción 

La función `predict` se utiliza para predecir las probabilidades de distribución para el modelo multinomial. Los resultados de la predicción no se muestran aquí, pero se pueden obtener utilizando la función `predict`.
    

```{r, eval=TRUE}
model <- readRDS("Recursos/04_Multinomial/Multinomial_model.rds")
predict(model,type = "probs") %>%  
  data.frame() %>% select(1:3) %>% tba()
```

## Construcción de la base censal con predicciones del modelo multinomial

Para la creación de la base se siguen los siguientes pasos: 

1. **Lectura de Datos del Censo**: Lee los datos del censo de viviendas y personas desde un archivo en formato RDS llamado "02_censo_vivienda_personas.rds".

2. **Lectura del Modelo Multinomial Previamente Creado**: Lee el modelo multinomial previamente creado desde un archivo en formato RDS llamado "Multinomial_model.rds".

3. **Cálculo de Probabilidades**: Calcula las probabilidades de pertenecer a cada categoría de resultado utilizando el modelo multinomial. Estas probabilidades se almacenan en un nuevo DataFrame llamado "probabilidad".

## Construcción de la base censal con predicciones del modelo multinomial

4. **Copia de los Datos del Censo**: Crea una copia de los datos del censo de viviendas y personas en un nuevo `data.frame` llamado "censo_vivienda_pred". Esto se hace para trabajar con una copia de los datos originales sin alterar los datos originales.

5. **Cálculo de la Suma de Recuentos Predichos**: Calcula la suma de los recuentos predichos de personas en el censo y la muestra. Este cálculo utiliza la columna "pred_conteos" en el `data.frame` "censo_vivienda_pred".

6. **Identificación de Nombres de Columnas de Grupos de Edad**: Identifica las columnas en el `data.frame` "censo_vivienda_pred" que corresponden a los grupos de edad. Estas columnas se almacenan en la variable "var_grupo" para su posterior uso.

## Código de R para la creación de la base censal. 

```{r, eval=FALSE}
censo_vivienda <- 
  readRDS("Recursos/04_Multinomial/02_censo_vivienda_personas.rds") 

# Reading the previously created multinomial model.
model <- readRDS("Recursos/04_Multinomial/Multinomial_model.rds")

# Calculate probabilities for each outcome category using the model.
probabilidad <-
  predict(model, type = "probs") %>% as.data.frame() %>%
  select_all(~paste0(.,"_prob")) %>% 
  mutate(PROV_ID = as.character(1:7))
```

## Código de R para la creación de la base censal. 

```{r, eval=FALSE}
# Create a copy of the census data to work with.
censo_vivienda_pred <- censo_vivienda 

# Calculate the sum of predicted counts.
sum(censo_vivienda_pred$pred_conteos)

# Identify column names corresponding to age groups.
var_grupo <- grep(x = names(censo_vivienda_pred),
                  pattern = "*_GRUPO\\d{,2}_sum$",
                  value = TRUE)
```

## Resultados para la  Provincia **1**

**Filtrado de Datos del Censo para PROV_ID = 1 y Categorías Específicas de 'greenpoint2'**: Filtra los datos del censo para aquellas observaciones donde el valor de la columna "PROV_ID" es igual a "1" y la columna "greenpoint2" tiene uno de los siguientes valores: "Sin informacion pero n>=0" o "Sin informacion pero n>0". Los resultados de este filtro se almacenan en un nuevo DataFrame llamado "PROV_1".

```{r, eval=FALSE}
# Filter census data for PROV_ID = 1 and specific
# greenpoint2 categories.
PROV_1 <- censo_vivienda_pred %>% filter(
  PROV_ID == "1" ,
  greenpoint2 %in% c("Sin informacion pero n>=0",
                     "Sin informacion pero  n>0")
)
```

## Resultados para la  Provincia **1**

**Cálculo de Recuentos Predichos para Grupos de Edad en PROV_1**: Calcula los recuentos predichos para cada grupo de edad en "PROV_1". Utiliza la matriz de recuentos predichos "pred_conteos" en "PROV_1" y multiplica esta matriz por la matriz de probabilidades de pertenecer a cada grupo de edad específico en la provincia 1. Los resultados se almacenan en las columnas correspondientes a grupos de edad.


```{r, eval=FALSE}
# Calculate predicted counts for each age group in PROV_1.
PROV_1[, var_grupo] <-
  matrix(PROV_1$pred_conteos, nrow = nrow(PROV_1)) %*%
  matrix(as.numeric(probabilidad[1, paste0(var_grupo, "_prob")]),
         ncol = 40)
```

## Resultados para la  Provincia **1**

**Cálculo del Límite Inferior de Recuentos Predichos para Grupos de Edad en PROV_1**: Calcula el límite inferior de los recuentos predichos para cada grupo de edad en "PROV_1". Utiliza la matriz de límites inferiores "MEInf_pred_conteos" en "PROV_1" y realiza una operación similar a la anterior para obtener los recuentos inferiores para cada grupo de edad.

```{r, eval=FALSE}
# Calculate lower bound of predicted counts for each 
# age group in PROV_1.
PROV_1[, paste0(var_grupo, "_MEInf")] <-
  matrix(PROV_1$MEInf_pred_conteos, nrow = nrow(PROV_1)) %*%
  matrix(as.numeric(probabilidad[1, paste0(var_grupo, "_prob")]),
         ncol = 40)
```

## Resultados para la  Provincia **1**

**Cálculo del Límite Superior de Recuentos Predichos para Grupos de Edad en PROV_1**: Calcula el límite superior de los recuentos predichos para cada grupo de edad en "PROV_1". Utiliza la matriz de límites superiores "MESup_pred_conteos" en "PROV_1" y realiza una operación similar a la anterior para obtener los recuentos superiores para cada grupo de edad.


```{r, eval=FALSE}
# Calculate upper bound of predicted counts for 
# each age group in PROV_1.
PROV_1[, paste0(var_grupo, "_MESup")] <-
  matrix(PROV_1$MESup_pred_conteos, nrow = nrow(PROV_1)) %*%
  matrix(as.numeric(probabilidad[1, paste0(var_grupo, "_prob")]),
         ncol = 40)
```

**Ten en cuenta que el código está etiquetado se repite para cada una delas provincias.**

## Consolidando la base con las proviancias 

Combinar los resultados de las provincias con información faltante.

```{r, eval=FALSE}
prov_sin_informacion <-
  list(
    PROV_1,
    PROV_2,
    PROV_3,
    PROV_4,
    PROV_5,
    PROV_6,
    PROV_7) %>% bind_rows()
```



## Validaciones

1. Filtrado y Recopilación de Datos para Todas las Provincias con Información Faltante (Missing Information)

```{r, eval=FALSE}
# Filter and gather data for all provinces with missing information
PROV_todas <-
  censo_vivienda_pred %>% 
  filter(greenpoint2 %in% c("Sin informacion pero n>=0",
                            "Sin informacion pero  n>0"))
```


2. Cálculo de Sumas por Fila y Suma Total de Recuentos Predichos para Provincias con Información Faltante

```{r, eval=FALSE}
# Calculate row sums and total sum of predicted counts for provinces with missing information
rowSums(prov_sin_informacion[, var_grupo])
sum(prov_sin_informacion[, var_grupo])
sum(PROV_todas$pred_conteos)
```

## Validaciones

3. **Filtrado de Datos para Provincias con Información del Censo Completa (Censada)**: Filtra los datos de censo para todas las provincias donde el valor de la columna "greenpoint2" no corresponde a "Sin informacion pero n>=0" ni "Sin informacion pero n>0". Los resultados se almacenan en un nuevo DataFrame llamado "PROV_censada".

```{r, eval=FALSE}
# Filter data for provinces with complete census information
PROV_censada <-
  censo_vivienda_pred %>% 
  filter(!greenpoint2 %in% c("Sin informacion pero n>=0",
                              "Sin informacion pero  n>0"))
```

## Validaciones

- Inicializar columnas para los límites inferior y superior de los recuentos previstos

```{r, eval=FALSE}
# in provinces with complete census
PROV_censada[,paste0(var_grupo, "_MEInf")] <- 0
PROV_censada[,paste0(var_grupo, "_MESup")] <- 0

# Combine data frames of provinces with complete and missing information for each age group
censo_vivienda_grupo_edad <-
  bind_rows(PROV_censada, prov_sin_informacion) %>%
  dplyr::select(un_ID,
         var_grupo,
         paste0(var_grupo, "_MEInf"),
         paste0(var_grupo, "_MESup"))
```

## Guardar resultados 

```{r, eval=FALSE}
# Inner join the census data with the grouped age data and save the result
readRDS("Recursos/04_Multinomial/04_censo_vivienda_personas.rds") %>% 
  inner_join(censo_vivienda_grupo_edad) %>% 
  saveRDS("Recursos/04_Multinomial/05_censo_vivienda_personas_grupo_edad.rds")
```


## Estimaciones Agregadas modelo multinomial. 

Para el proceso se crearon  las siguientes funciones: 

- **plot_piramide_pob**: Genera un gráfico de pirámide de población con barras e intervalos de confianza.

- **piramide_pob**: Calcula y visualiza la pirámide de población a partir de los datos del censo.

```{r}
source("Recursos/04_Multinomial/01_Agregados.R")
censo_vivienda <- 
  readRDS("Recursos/04_Multinomial/05_censo_vivienda_personas_grupo_edad.rds")
```

## Predicción del total de personas por rango de edad y sexo

```{r, piramide1, eval=FALSE}
piramide_pob(
  censo = censo_vivienda,
  Plot = FALSE,
  agrega = NULL,
  filtro = NULL
) %>% head(15) %>% tba()
```
\tiny

```{r, piramide1, eval=TRUE, echo=FALSE}
```
## Predicción del total de personas por rango de edad y sexo

```{r, eval=FALSE}
p1 <- piramide_pob(
  censo = censo_vivienda,
  Plot = TRUE,
  agrega = NULL,
  filtro = NULL)
```

## Predicción del total de personas por rango de edad y sexo
![Resultado nacional](Recursos/04_Multinomial/06_fig_piramide_nacional.png){width="350"}

## Predicción del total de personas por rango de edad y sexo

```{r, eval=FALSE}
p1 <- piramide_pob(
  censo = censo_vivienda,
  Plot = TRUE,
  agrega = "PROV_ID",
  filtro = "1"
)
```

## Predicción del total de personas por rango de edad y sexo
![Resultado para la provincia 1 ](Recursos/04_Multinomial/07_fig_piramide_prov1.png){width="350"}


## Predicción del total de personas por rango de edad y sexo

```{r, eval=FALSE}
p1 <- piramide_pob(
  censo = censo_vivienda,
  Plot = TRUE,
  agrega = "CANT_ID",
  filtro = "101"
)
```

## Predicción del total de personas por rango de edad y sexo

![Resultado para la canton 101 ](Recursos/04_Multinomial/08_fig_piramide_canton101.png){width="350"}


## ¡Gracias!

::: yellow
*Email*: [andres.gutierrez\@cepal.org](mailto:andres.gutierrez@cepal.org){.email}
:::
